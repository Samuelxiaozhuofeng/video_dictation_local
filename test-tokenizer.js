/**
 * Simple test script for the tokenizer
 * Run with: node test-tokenizer.js
 */

// Mock the tokenizer logic for testing
const TokenType = {
  WORD: 'WORD',
  PUNCTUATION: 'PUNCTUATION',
  SPACE: 'SPACE'
};

const tokenizeText = (text) => {
  const tokens = [];
  let index = 0;
  
  // Unicode-aware regex pattern
  const pattern = /([\p{L}\p{N}]+(?:'[\p{L}]+)?)|([^\p{L}\p{N}\s])|(\s+)/gu;
  
  let match;
  while ((match = pattern.exec(text)) !== null) {
    if (match[1]) {
      tokens.push({
        type: TokenType.WORD,
        value: match[1],
        index: index++
      });
    } else if (match[2]) {
      tokens.push({
        type: TokenType.PUNCTUATION,
        value: match[2],
        index: index++
      });
    } else if (match[3]) {
      tokens.push({
        type: TokenType.SPACE,
        value: match[3],
        index: index++
      });
    }
  }
  
  return tokens;
};

// Test cases
const testCases = [
  {
    name: "Spanish with Ã±",
    text: "El niÃ±o estÃ¡ en EspaÃ±a.",
    expected: ["El", "niÃ±o", "estÃ¡", "en", "EspaÃ±a"]
  },
  {
    name: "Spanish with accents",
    text: "Â¿CÃ³mo estÃ¡s? Â¡Muy bien!",
    expected: ["CÃ³mo", "estÃ¡s", "Muy", "bien"]
  },
  {
    name: "Spanish names",
    text: "JosÃ© y MarÃ­a estÃ¡n en BogotÃ¡.",
    expected: ["JosÃ©", "y", "MarÃ­a", "estÃ¡n", "en", "BogotÃ¡"]
  },
  {
    name: "Spanish question",
    text: "Â¿DÃ³nde estÃ¡ la seÃ±orita MartÃ­nez?",
    expected: ["DÃ³nde", "estÃ¡", "la", "seÃ±orita", "MartÃ­nez"]
  },
  {
    name: "English with contractions",
    text: "Don't worry, it's okay!",
    expected: ["Don't", "worry", "it's", "okay"]
  },
  {
    name: "Mixed punctuation",
    text: "Hello, world! How are you?",
    expected: ["Hello", "world", "How", "are", "you"]
  }
];

console.log("ğŸ§ª Testing Tokenizer with Unicode Support\n");
console.log("=".repeat(60));

let passed = 0;
let failed = 0;

testCases.forEach((testCase, i) => {
  console.log(`\nTest ${i + 1}: ${testCase.name}`);
  console.log(`Input: "${testCase.text}"`);
  
  const tokens = tokenizeText(testCase.text);
  const words = tokens.filter(t => t.type === TokenType.WORD).map(t => t.value);
  
  console.log(`Expected words: [${testCase.expected.join(', ')}]`);
  console.log(`Actual words:   [${words.join(', ')}]`);
  
  const isCorrect = JSON.stringify(words) === JSON.stringify(testCase.expected);
  
  if (isCorrect) {
    console.log("âœ… PASSED");
    passed++;
  } else {
    console.log("âŒ FAILED");
    failed++;
  }
  
  // Show all tokens for debugging
  console.log("All tokens:");
  tokens.forEach(token => {
    const typeSymbol = token.type === TokenType.WORD ? 'ğŸ“' : 
                       token.type === TokenType.PUNCTUATION ? 'ğŸ”£' : 'âµ';
    console.log(`  ${typeSymbol} ${token.type.padEnd(12)} "${token.value}"`);
  });
});

console.log("\n" + "=".repeat(60));
console.log(`\nğŸ“Š Results: ${passed} passed, ${failed} failed out of ${testCases.length} tests`);

if (failed === 0) {
  console.log("ğŸ‰ All tests passed! Unicode support is working correctly.");
} else {
  console.log("âš ï¸  Some tests failed. Please review the implementation.");
}

